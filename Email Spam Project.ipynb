{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis to Detect Spam Emails\n",
    "\n",
    "Data used: spam.csv\n",
    "\n",
    "This project focuses on building a supervised machine learning model that determines the probability any given email is spam mail or not.  The project relies on techniques found in text mining practices and the use of the pandas, numpy and sklearn python libraries.  The goal of the project is to build a generally reliable model as well as explore some of the key features and commonalities between spam mail samples to better understand the breadth (or lack thereof) of complexity and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = pd.read_csv('spam.csv')\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0) # converts spam text to binary\n",
    "print(spam_data.shape) # 5572 samples\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['target'].mean()*100 # 13.4% of the cases are spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Length of Non-Spam Emails: 71.0\n",
      "Avg. Length of Spam Emails: 139.0\n"
     ]
    }
   ],
   "source": [
    "def length_of_emails():\n",
    "    temp = spam_data.copy()\n",
    "    temp['length'] = temp['text'].str.len()\n",
    "    average_length = temp.groupby('target')['length'].agg('mean').values\n",
    "    return average_length[0], average_length[1]\n",
    "not_s,s = length_of_emails()\n",
    "print('Avg. Length of Non-Spam Emails: ' + str(round(not_s)))\n",
    "print('Avg. Length of Spam Emails: ' + str(round(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Number of digits in Non-Spam Emails: 0.3\n",
      "Avg. Number of digits in Spam Emails: 15\n"
     ]
    }
   ],
   "source": [
    "def avg_digits():\n",
    "    import re\n",
    "    spam_data['digits_count'] = spam_data['text'].apply(lambda row: len(re.findall(r'(\\d)', row)))\n",
    "    average_digits = spam_data.groupby('target')['digits_count'].agg('mean').values\n",
    "    return average_digits[0], average_digits[1]\n",
    "not_s_d, s_d = avg_digits()\n",
    "print('Avg. Number of digits in Non-Spam Emails: ' + str(round(not_s_d,2)))\n",
    "print('Avg. Number of digits in Spam Emails: ' + str(int(s_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Number of non-word characters in Non-Spam Emails: 17\n",
      "Avg. Number of non-word characters in Spam Emails: 29\n"
     ]
    }
   ],
   "source": [
    "def non_words():\n",
    "    import re\n",
    "    t = spam_data.copy()\n",
    "    t['not_words'] = t['text'].apply(lambda row: len(re.findall(r'\\W', row)))\n",
    "    avg_nowords = t.groupby('target')['not_words'].agg('mean').values\n",
    "    return avg_nowords[0], avg_nowords[1]\n",
    "not_s_w, s_w = non_words()\n",
    "print('Avg. Number of non-word characters in Non-Spam Emails: ' + str(int(not_s_w)))\n",
    "print('Avg. Number of non-word characters in Spam Emails: ' + str(int(s_w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is noted to be skewed with less than 14% of the cases to be spam (target = 1).  Surprisingly, at first glance, spam emails are longer than non-spam emails.  Moreover, it appears the number of digits in an email might be good indicator of spam mail along with larger proportions on non-word characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build out Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Alpha: Naive Bayes Classifier\n",
    "\n",
    "This model utilizes and bag of words approach as to compare the vocabulary of each case to the general use of the entire vocabulary for each target bucket.  It then takes the Bayes Algorithm to conclude the probabilites of the words in each sample being part of either bucket. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9720812182741116\n",
      "[[1196    0]\n",
      " [  11  186]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def NB_model():\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    model_nb = MultinomialNB(alpha=0.1).fit(X_train_vectorized, y_train)\n",
    "    predictions = model_nb.predict(vect.transform(X_test))\n",
    "    score = roc_auc_score(y_test, predictions)\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    return score, cm\n",
    "NB_auc, NB_cm = NB_model()\n",
    "print('AUC Score: ' + str(NB_auc))\n",
    "print(NB_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first model, it appears that the Naive Bayes Classifier does a great job determining if the email is not spam, having a false positive score of zero.  Our false negative score is also relatively low with eleven cases misinterpreted.  Overall a good baseline to compare to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Beta: TF-IDF Vectorized and Tuned Naive Bayes Classifier\n",
    "\n",
    "This model uses the TF-IDF method to assign value to the relevancy of each word in an email.  TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "This model applies the TF-IDF method to organizing the data and employs a smooting alpha of 0.1 while ignoring terms that have a frequency less than three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten smallest tfidf:\n",
      "\n",
      "                           tfidf\n",
      "features                        \n",
      "aaniye        0.0744745235430759\n",
      "athletic      0.0744745235430759\n",
      "chef          0.0744745235430759\n",
      "companion     0.0744745235430759\n",
      "courageous    0.0744745235430759\n",
      "dependable    0.0744745235430759\n",
      "determined    0.0744745235430759\n",
      "exterminator  0.0744745235430759\n",
      "healer        0.0744745235430759\n",
      "listener      0.0744745235430759\n",
      "\n",
      "Ten largest tfidf:\n",
      "\n",
      "          tfidf\n",
      "features       \n",
      "146tf150p   1.0\n",
      "645         1.0\n",
      "anything    1.0\n",
      "anytime     1.0\n",
      "beerage     1.0\n",
      "done        1.0\n",
      "er          1.0\n",
      "havent      1.0\n",
      "home        1.0\n",
      "lei         1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_view(): # returns the ten least and ten most relevant indicators of spam \n",
    "    vect = TfidfVectorizer().fit(X_train)\n",
    "    feature_names = np.array(vect.get_feature_names()).reshape(-1, 1)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    tfidf_values = X_train_vectorized.max(0).toarray()[0].reshape(-1, 1)\n",
    "    df = pd.DataFrame(data=np.hstack((feature_names, tfidf_values)), columns=['features', 'tfidf'])\n",
    "    smalldf = df.sort_values(by=['tfidf', 'features']).set_index('features')[:10]\n",
    "    largestdf = df.sort_values(by=['tfidf', 'features'], ascending=[False, True]).set_index('features')[:10]\n",
    "    return smalldf, largestdf\n",
    "s, l = tfidf_view()\n",
    "print('Ten smallest tfidf:\\n')\n",
    "print(s)\n",
    "print('\\nTen largest tfidf:\\n')\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9954968337775665\n",
      "[[1196    0]\n",
      " [  23  174]]\n"
     ]
    }
   ],
   "source": [
    "def tfidf_NB_model():\n",
    "    vect = TfidfVectorizer(min_df=3).fit(X_train) # ignores frequencies lees than 3\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    clf = MultinomialNB(alpha=0.1).fit(X_train_vectorized, y_train) # alpha smooths out the make sure there is never a probability of zero\n",
    "    y_score = clf.predict_proba(X_test_vectorized)[:, 1]\n",
    "    pred_labels = []\n",
    "    for s in y_score:\n",
    "        if s > 0.5:\n",
    "            pred_labels.append(1)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "    y_pred = np.array(pred_labels)\n",
    "    score = roc_auc_score(y_test, y_score)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return score, cm\n",
    "NB2_auc, NB2_cm = tfidf_NB_model()\n",
    "print('AUC Score: ' + str(NB2_auc))\n",
    "print(NB2_cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model appears to do overall well in predicting spam, however, its AUC score is less than our baseline model and has a slightly larger false negative count. Further tuning might be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Gamma: SVM model with TF-IDF Applied\n",
    "\n",
    "This model fits and transforms the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 5, using this document-term matrix and an additional feature, the length of document (number of characters). The data is fitted to a Support Vector Classification model with regularization C=10000. \n",
    "This function returns the AUC score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9951106055718724\n",
      "[[1193    3]\n",
      " [  16  181]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "\n",
    "def SVC_model():\n",
    "    spam_data['length_of_doc'] = spam_data['text'].str.len()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(spam_data.drop('target', axis=1), spam_data['target'], random_state=0)\n",
    "    vect = TfidfVectorizer(min_df=5).fit(X_train['text'])\n",
    "    X_train_vectorized = vect.transform(X_train['text'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['length_of_doc'])\n",
    "    clf = SVC(C=10000).fit(X_train_vectorized, y_train)\n",
    "    X_test_vectorized = vect.transform(X_test['text'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['length_of_doc'])\n",
    "    y_score = clf.decision_function(X_test_vectorized)\n",
    "    score = roc_auc_score(y_test, y_score)\n",
    "    p = clf.predict(X_test_vectorized)\n",
    "    cm = confusion_matrix(y_test, p)\n",
    "    return score, cm\n",
    "SVC_auc, SVC_cm = SVC_model()\n",
    "print('AUC Score: ' + str(SVC_auc))\n",
    "print(SVC_cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model results in similar results as the last model, posting a similar AUC score and confusion matrix. The number of false negatives droped by seven while false positives increased by three (most likely due to the high level of regularization that is being applied with a C of 10000.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Delta: Tuned Logistic Regression\n",
    "\n",
    "Fits and transforms the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than 5 and using word n-grams from n=1 to n=3 (unigrams, bigrams, and trigrams).\n",
    "\n",
    "Adds the following additional features:\n",
    "1) the length of document (number of characters)\n",
    "2) number of digits per document\n",
    "\n",
    "Fits a Logistic Regression model with regularization C=100. Then computes the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "This function returns the AUC score as a float and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9678709064054463\n",
      "[[1192    4]\n",
      " [  12  185]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def LR_model():\n",
    "    import re\n",
    "    temp = spam_data.copy()\n",
    "    temp['length_of_doc'] = temp['text'].str.len() # overall length of email\n",
    "    temp['digits_count'] = temp['text'].apply(lambda row: len(re.findall(r'(\\d)', row))) # finds digits per email\n",
    "    X_train, X_test, y_train, y_test = train_test_split(temp.drop('target', axis=1), temp['target'], random_state=0)\n",
    "    # resplit train/test to include new features\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1, 3)).fit(X_train['text'])\n",
    "    X_train_vectorized = vect.transform(X_train['text'])\n",
    "    X_test_vectorized = vect.transform(X_test['text'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['length_of_doc'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['digits_count'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['length_of_doc'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['digits_count'])\n",
    "    \n",
    "    clf = LogisticRegression(C=100).fit(X_train_vectorized, y_train)\n",
    "    y_score = clf.predict(X_test_vectorized)\n",
    "    cm = confusion_matrix(y_test, y_score)\n",
    "    score = roc_auc_score(y_test, y_score)\n",
    "    return score, cm\n",
    "\n",
    "LR_auc, LR_cm = LR_model()\n",
    "print('AUC Score: ' + str(LR_auc))\n",
    "print(LR_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performs very well but at a less effective degree as the SVM and and Naive Bayes Models.  Second smallest false negative count and highest false positive count.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Epsilon: Logistic Regression With Count Vectorizer\n",
    "\n",
    "Fits and transforms the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than 5 and using character n-grams from n=2 to n=5. Passes in analyzer='char_wb' which creates character n-grams only from text inside word boundaries. This makes the model more robust to spelling mistakes.\n",
    "\n",
    "Uses this document-term matrix and the following additional features:\n",
    "\n",
    "- The length of document (number of characters)\n",
    "- Number of digits per document\n",
    "- Number of non-word characters (anything other than a letter, digit or underscore.)\n",
    "\n",
    "Fits a Logistic Regression model with regularization C=100. Then computes the area under the curve (AUC) score using the transformed test data and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9788593110707434\n",
      "[[1194    2]\n",
      " [   8  189]]\n"
     ]
    }
   ],
   "source": [
    "def LR2_model():\n",
    "    import re\n",
    "    temp = spam_data.copy()\n",
    "    temp['length_of_doc'] = temp['text'].str.len()\n",
    "    temp['digit_count'] = spam_data['text'].apply(lambda row: len(re.findall(r'\\d', row)))\n",
    "    temp['non_word_char_count'] = temp['text'].apply(lambda row: len(re.findall(r'\\W', row)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(temp.drop('target', axis=1), temp['target'], random_state=0)\n",
    "    \n",
    "    vect = CountVectorizer(min_df=5, ngram_range=(2, 5), analyzer='char_wb').fit(X_train['text'])\n",
    "    X_train_vectorized = vect.transform(X_train['text'])\n",
    "    X_test_vectorized = vect.transform(X_test['text'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['length_of_doc'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['digit_count'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['non_word_char_count'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['length_of_doc'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['digit_count'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['non_word_char_count'])\n",
    "    clf = LogisticRegression(C=100).fit(X_train_vectorized, y_train)\n",
    "    y_score = clf.predict(X_test_vectorized)\n",
    "    score = roc_auc_score(y_test, y_score)\n",
    "    cm = confusion_matrix(y_test, y_score)\n",
    "    return score, cm\n",
    "\n",
    "LR2_auc, LR2_cm = LR2_model()\n",
    "print('AUC Score: ' + str(LR2_auc))\n",
    "print(LR2_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By controlling for spelling errors and increasing the n-gram range, the Logistic Regression improved from the previous run, earning a higher AUC socre and small false positive and Negative Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Summarize Models and Discern Best Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>NB</th>\n",
       "      <th>LR2</th>\n",
       "      <th>SVC</th>\n",
       "      <th>NB2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AUC Score</td>\n",
       "      <td>0.967871</td>\n",
       "      <td>0.972081</td>\n",
       "      <td>0.978859</td>\n",
       "      <td>0.995111</td>\n",
       "      <td>0.995497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Confusion Matrix</td>\n",
       "      <td>[[1192, 4], [12, 185]]</td>\n",
       "      <td>[[1196, 0], [11, 186]]</td>\n",
       "      <td>[[1194, 2], [8, 189]]</td>\n",
       "      <td>[[1193, 3], [16, 181]]</td>\n",
       "      <td>[[1196, 0], [23, 174]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      LR                      NB  \\\n",
       "AUC Score                       0.967871                0.972081   \n",
       "Confusion Matrix  [[1192, 4], [12, 185]]  [[1196, 0], [11, 186]]   \n",
       "\n",
       "                                    LR2                     SVC  \\\n",
       "AUC Score                      0.978859                0.995111   \n",
       "Confusion Matrix  [[1194, 2], [8, 189]]  [[1193, 3], [16, 181]]   \n",
       "\n",
       "                                     NB2  \n",
       "AUC Score                       0.995497  \n",
       "Confusion Matrix  [[1196, 0], [23, 174]]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = [NB_auc, NB_cm]\n",
    "NB2 = [NB2_auc, NB2_cm]\n",
    "SVC = [SVC_auc, SVC_cm]\n",
    "LR = [LR_auc, LR_cm]\n",
    "LR2 = [LR2_auc, LR2_cm]\n",
    "d = zip(NB, NB2, SVC, LR, LR2)\n",
    "df = pd.DataFrame(d, index = ['AUC Score', 'Confusion Matrix'])\n",
    "c = ['NB', 'NB2', 'SVC', 'LR', 'LR2']\n",
    "df.columns = c\n",
    "df = df.sort_values(by= 'AUC Score', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Conclusions\n",
    "\n",
    "In analyzing the outputs of each of these models, it can be concluded that either the Support Vector or Tuned Naive Bayes Models would be most suitable for prediciting spam emails given their AUC scores are nearly identical and the highest of the five models.  Between the two, there is a tradeoff the lies in their precision and recall scores that are up to the user to discern their preference. The tuned Naive Bayes is a stong candidate as it has zero false positve counts, however, it has an overall lower accuracy than the SVC model (19 mistakes vs 23). In practice, these differences are minute but important if we believe that having false positives can be especially harmful to the implimentation of the model in an uncontrolled setting which.  In all, either are good candidates and an argument can be made for all five candidates, given their AUC scores are within 0.03 of each other's AUC scores.\n",
    "\n",
    "General conclusions also support ignoring word frequencies less than 3 to be a good benchmark as well as adding in a regularizing penalty to smooth out inconsistencies between things like spelling or outliers.  Length of Doucument also appears to be a potential indicator but is not used in the Naive Bayes models, suggesting that is might play a minor role in a more finely tuned model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
